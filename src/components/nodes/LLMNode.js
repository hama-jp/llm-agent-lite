import { createNodeDefinition } from './types.js';

/**
 * LLMç”Ÿæˆãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œå‡¦ç†
 * @param {Object} node - ãƒãƒ¼ãƒ‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
 * @param {Object} inputs - å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
 * @param {Object} context - å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
 * @returns {Promise<string>} LLMã‹ã‚‰ã®å¿œç­”
 */
async function executeLLMNode(node, inputs, context) {
  const temperature = node.data.temperature || 0.7;
  const model = node.data.model;
  const provider = node.data.provider || 'openai';
  const systemPrompt = node.data.systemPrompt || null;
  const maxTokens = node.data.maxTokens || null;
  
  // å…¥åŠ›ã‚’ãã®ã¾ã¾LLMã«é€ä¿¡
  const inputValues = Object.values(inputs).filter(v => v !== undefined && v !== null);
  if (inputValues.length === 0) {
    throw new Error('LLMãƒãƒ¼ãƒ‰ã«å…¥åŠ›ãŒã‚ã‚Šã¾ã›ã‚“');
  }
  
  // æœ€åˆã®å…¥åŠ›å€¤ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ä½¿ç”¨
  const finalPrompt = String(inputValues[0]);
  
  context.addLog('info', `LLMã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ${finalPrompt.substring(0, 100)}...`, node.id, { 
    prompt: finalPrompt,
    systemPrompt,
    model,
    temperature,
    provider
  });

  try {
    // llmServiceã‚’å‹•çš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¾ªç’°ä¾å­˜ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    const { default: llmService } = await import('../../services/llmService.js');
    context.addLog('debug', `llmServiceã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ`, node.id);
    
    const currentSettings = llmService.loadSettings();
    context.addLog('debug', `ç¾åœ¨ã®è¨­å®š`, node.id, { currentSettings });
    
    const nodeSpecificOptions = {
      provider,
      model,
      temperature,
      apiKey: currentSettings.apiKey,
      baseUrl: currentSettings.baseUrl,
      maxTokens: maxTokens || currentSettings.maxTokens  // ãƒãƒ¼ãƒ‰è¨­å®šã‚’å„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
    };
    context.addLog('debug', `ãƒãƒ¼ãƒ‰å›ºæœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³`, node.id, { 
      nodeSpecificOptions,
      maxTokensSource: maxTokens ? 'node-specific' : 'system-default',
      nodeMaxTokens: maxTokens,
      systemMaxTokens: currentSettings.maxTokens,
      finalMaxTokens: maxTokens || currentSettings.maxTokens
    });

    context.addLog('debug', `llmService.sendMessageå‘¼ã³å‡ºã—é–‹å§‹`, node.id);
    
    // è©³ç´°ãªAPIãƒ¬ã‚¹ãƒãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ†ãƒ³ãƒãƒ©ãƒªãƒ¼ãªä¿®æ­£
    const result = await llmService.sendMessage(finalPrompt, systemPrompt, nodeSpecificOptions);
    
    // ãƒ–ãƒ©ã‚¦ã‚¶ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®æœ€æ–°ãƒ­ã‚°ï¼ˆAPIãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼‰ã‚’å–å¾—ã‚’è©¦ã¿ã‚‹
    setTimeout(() => {
      if (console.log.lastCall && console.log.lastCall.includes('LLM Response:')) {
        context.addLog('debug', `ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ - ç›´æ¥F12ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„`, node.id);
      }
    }, 100);
    
    context.addLog('debug', `llmService.sendMessageå‘¼ã³å‡ºã—å®Œäº†`, node.id, { 
      resultType: typeof result, 
      resultLength: result?.length,
      result: result,
      isEmptyResponse: result === 'ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã™'
    });
    
    context.addLog('info', `LLMã‹ã‚‰ã®å¿œç­”ã‚’å—ä¿¡ã—ã¾ã—ãŸ`, node.id, { response: result?.substring(0, 100) + '...' });
    
    return result;
  } catch (error) {
    context.addLog('error', `LLMãƒãƒ¼ãƒ‰å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: ${error.message}`, node.id, { error: error.stack });
    throw error;
  }
}

/**
 * LLMç”Ÿæˆãƒãƒ¼ãƒ‰ã®å®šç¾©
 * AIãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹
 */
export const LLMNode = createNodeDefinition(
  'LLM Generation',
  'ğŸ¤–',
  'blue',
  ['input'], // å…¥åŠ›ãƒãƒ¼ãƒˆ: input
  ['output'], // å‡ºåŠ›ãƒãƒ¼ãƒˆ: output
  {
    temperature: 1.0,
    model: 'gpt-5-nano',
    systemPrompt: '',
    maxTokens: 50000
  },
  executeLLMNode, // å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰
  {
    description: 'Generate text using AI language models. Supports system prompts, temperature settings, and model selection.',
    category: 'ai'
  }
);

export default LLMNode;